# Неделя 5: Нейронные сети и обзор методов
## Нейронные сети
  * _Слайды_ [Нейронные сети](week_5/materials/neural-Slides.pdf): _однослойная нейронная сеть, многослойная нейронная сеть, оптимизация параметров нейронной сети, регуляризация и прореживание нейронной сети_

### [Assignment](week_5/assignment_1/task_nn.ipynb) Нейронные сети
В этом задании вы будете настраивать двуслойную нейронную сеть для решения задачи многоклассовой классификации. Предлагается выполнить процедуры загрузки и разбиения входных данных, обучения сети и подсчета ошибки классификации. Предлагается определить оптимальное количество нейронов в скрытом слое сети. Нужно так подобрать число нейронов, чтобы модель была с одной стороны несложной, а с другой стороны давала бы достаточно точный прогноз и не переобучалась. Цель задания -- показать, как зависит точность и обучающая способность сети от ее сложности.

Для решения задачи многоклассовой классификации предлагается воспользоваться библиотекой построения нейронных сетей pybrain. Библиотека содержит основные модули инициализации двуслойной нейронной сети прямого распространения, оценки ее параметров с помощью метода обратного распространения ошибки (backpropagation) и подсчета ошибки.

## Байесовская классификация и регрессия
 * _Слайды_ [Байесовская классификация](week_5/materials/bayes-classification-Slides.pdf): _спам-фильтры и наивный байесовский классификатор, байесовский классификатор, восстановление распределений, минимизация риска и анализ функции потерь_
 
### [Assignment](week_5/assignment_2/naiv_bayes.ipynb) Выбор семейства распределений в наивном байесе
Вам предлагается выяснить, какое распределение лучше использовать в наивном байесовском классификаторе в зависимости от вида признаков. Будем исследовать стандартные датасеты digits и breast_cancer. Будем оценивать качество работы наивных байесовских классификаторов на этих двух датасетах по кросс-валидации. Для сравнения предлагается использовать BernoulliNB, MultinomialNB и GaussianNB.

## Метрические алгоритмы и SVM
 * _Слайды_ [Метрические алгоритмы и SVM](week_5/materials/knn-Slides.pdf): _метод k ближайших соседей, настройка параметров к kNN, метрики, проклятие размерности, рекомендации фильмов с помощью kNN, метод опорных векторов (SVM), ядра в методы опорных векторов_
 
### [Assignment](week_5/assignment_3/1NN_vs_RandomForest.ipynb) 1NN против RandomForest
В этом задании будет использоваться датасет digits. Реализуем самостоятельно метод одного ближайшего соседа с евклидовой метрикой для задачи классификации. Затем обучим на обучающей выборке случайный лес RandomForestClassifier(n_estimators=1000) и сравним с 1NN.

## Теорема Байеса в машинном обучении
 * _Слайды_ [Теорема Байеса](week_5/materials/bayes-theorem-Slides.pdf)
